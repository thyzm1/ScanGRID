#!/bin/bash

# Script d'installation et de configuration d'Ollama pour ScanGRID
# √Ä ex√©cuter sur le Raspberry Pi

set -e

echo "ü§ñ Installation et configuration d'Ollama pour ScanGRID"
echo "========================================================"

# 1. V√©rifier si Ollama est d√©j√† install√©
if command -v ollama &> /dev/null; then
    echo "‚úÖ Ollama est d√©j√† install√© (version: $(ollama --version))"
else
    echo "üì• Installation d'Ollama..."
    curl -fsSL https://ollama.ai/install.sh | sh
    echo "‚úÖ Ollama install√© avec succ√®s"
fi

# 2. Configurer Ollama comme service systemd (si possible)
echo ""
echo "üîß Configuration du service Ollama..."
if command -v systemctl &> /dev/null; then
    # Cr√©er le fichier de service s'il n'existe pas
    if [ ! -f /etc/systemd/system/ollama.service ]; then
        echo "üìù Cr√©ation du service systemd..."
        sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=$USER
Group=$USER
Restart=always
RestartSec=3
Environment="PATH=/usr/local/bin:/usr/bin:/bin"

[Install]
WantedBy=default.target
EOF
        
        sudo systemctl daemon-reload
        sudo systemctl enable ollama
        echo "‚úÖ Service systemd cr√©√© et activ√©"
    else
        echo "‚úÖ Service systemd d√©j√† configur√©"
    fi
    
    # D√©marrer le service
    echo "üöÄ D√©marrage du service Ollama..."
    sudo systemctl start ollama
    sleep 2
    
    if sudo systemctl is-active --quiet ollama; then
        echo "‚úÖ Service Ollama actif"
    else
        echo "‚ö†Ô∏è  Le service n'a pas d√©marr√© correctement"
        sudo systemctl status ollama
    fi
else
    echo "‚ö†Ô∏è  systemd non disponible, d√©marrage manuel..."
    nohup ollama serve > /tmp/ollama.log 2>&1 &
    sleep 2
    echo "‚úÖ Ollama d√©marr√© en arri√®re-plan"
fi

# 3. T√©l√©charger le mod√®le llama3.2:1b
echo ""
echo "üì¶ T√©l√©chargement du mod√®le llama3.2:1b..."
if ollama list | grep -q "llama3.2:1b"; then
    echo "‚úÖ Mod√®le llama3.2:1b d√©j√† t√©l√©charg√©"
else
    echo "‚è¨ T√©l√©chargement en cours (~1.3 GB)..."
    ollama pull llama3.2:1b
    echo "‚úÖ Mod√®le t√©l√©charg√© avec succ√®s"
fi

# 4. Test du mod√®le
echo ""
echo "üß™ Test du mod√®le..."
RESPONSE=$(ollama run llama3.2:1b "R√©ponds simplement 'OK' si tu es fonctionnel" 2>&1 | head -n 5)
if [ $? -eq 0 ]; then
    echo "‚úÖ Mod√®le fonctionnel"
    echo "   R√©ponse: $RESPONSE"
else
    echo "‚ùå Erreur lors du test du mod√®le"
fi

# 5. R√©capitulatif
echo ""
echo "========================================================"
echo "üéâ Installation d'Ollama termin√©e !"
echo "========================================================"
echo ""
echo "üìä √âtat du syst√®me:"
if command -v systemctl &> /dev/null && sudo systemctl is-active --quiet ollama; then
    echo "   Service: ‚úÖ Actif (systemd)"
    echo "   Commandes utiles:"
    echo "     - Red√©marrer: sudo systemctl restart ollama"
    echo "     - Statut:     sudo systemctl status ollama"
    echo "     - Logs:       sudo journalctl -u ollama -f"
elif pgrep -x "ollama" > /dev/null; then
    echo "   Service: ‚úÖ Actif (processus manuel)"
    echo "   PID:     $(pgrep -x ollama)"
else
    echo "   Service: ‚ùå Non actif"
fi

echo ""
echo "üì¶ Mod√®les disponibles:"
ollama list

echo ""
echo "üîó Prochaines √©tapes:"
echo "   1. V√©rifiez que le backend a ollama install√©: cd backend && pip install ollama"
echo "   2. Red√©marrez ScanGRID: ./launch.sh ou pm2 reload ecosystem.config.js"
echo "   3. Testez l'am√©lioration de description dans l'interface web"
echo ""
