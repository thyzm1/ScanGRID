#!/bin/bash

# Fonction pour tuer les processus en quittant
cleanup() {
    echo ""
    echo "üõë Arr√™t des serveurs..."
    kill $(jobs -p) 2>/dev/null
    exit
}

# Pi√©ger le signal de sortie (Ctrl+C)
trap cleanup SIGINT SIGTERM

echo "üöÄ D√©marrage de ScanGRID..."

# 0. V√©rifier et d√©marrer Ollama
echo "ü§ñ V√©rification d'Ollama..."
if command -v ollama &> /dev/null; then
    if ! pgrep -x "ollama" > /dev/null; then
        echo "üîÑ D√©marrage d'Ollama..."
        nohup ollama serve > /dev/null 2>&1 &
        sleep 2
        echo "‚úÖ Ollama d√©marr√©"
    else
        echo "‚úÖ Ollama d√©j√† actif"
    fi
    # V√©rifier le mod√®le
    if ! ollama list | grep -q "llama3.2:1b"; then
        echo "‚ö†Ô∏è  Mod√®le llama3.2:1b non trouv√©. T√©l√©chargez-le avec:"
        echo "   ollama pull llama3.2:1b"
    fi
else
    echo "‚ö†Ô∏è  Ollama non install√©. L'am√©lioration IA ne sera pas disponible."
    echo "   Installez-le avec: curl -fsSL https://ollama.ai/install.sh | sh"
fi

# 1. D√©marrer le Backend (Port 8001)
echo "üì¶ Lancement du Backend (FastAPI)..."
cd backend
# V√©rifier si venv existe
if [ ! -d "venv" ]; then
    echo "‚ö†Ô∏è  Environnement virtuel non trouv√©. Cr√©ation..."
    python3 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
else
    source venv/bin/activate
fi

# D√©marrer le backend en arri√®re-plan
SCANGRID_DB_DIR=./data python -m uvicorn main:app --host 0.0.0.0 --port 8001 --reload &
BACKEND_PID=$!
echo "‚úÖ Backend d√©marr√© (PID: $BACKEND_PID)"

# 2. D√©marrer le Frontend (Vite)
echo "üíª Lancement du Frontend (React/Vite)..."
cd ../front
# Installer les d√©pendances si node_modules n'existe pas
if [ ! -d "node_modules" ]; then
    echo "üì¶ Installation des d√©pendances npm..."
    npm install
fi

# D√©marrer le frontend
npm run dev &
FRONTEND_PID=$!

echo "‚ú® Tout est pr√™t !"
echo "üëâ Frontend : http://localhost:5173"
echo "üëâ Backend  : http://localhost:8001"
echo "Appuyez sur Ctrl+C pour arr√™ter."

# Attendre ind√©finiment
wait
