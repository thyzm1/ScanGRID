# ü§ñ Configuration IA - Am√©lioration de Descriptions

## üìã Vue d'ensemble

Cette fonctionnalit√© permet d'am√©liorer automatiquement les descriptions de composants en utilisant **Ollama** avec le mod√®le ultra-l√©ger **llama3.2:1b** (parfait pour Raspberry Pi).

## üéØ Fonctionnalit√©s

- ‚ú® Bouton "Am√©liorer avec IA" dans l'√©diteur de bo√Ætes
- üöÄ G√©n√©ration de descriptions ultra-concises (max 25 mots)
- üîã Optimis√© pour Raspberry Pi (mod√®le 1B param√®tres)
- ‚ö° Rapide et √©conome en ressources

## ÔøΩ Installation Rapide (Automatis√©e)

### Option 1 : Script d'installation d√©di√© (Recommand√©)

```bash
# Sur votre Raspberry Pi
cd /Users/mathisdupont/ScanGRID

# Ex√©cuter le script d'installation Ollama
./setup_ollama.sh
```

Ce script fait tout automatiquement :
- ‚úÖ Installe Ollama
- ‚úÖ Configure le service systemd
- ‚úÖ T√©l√©charge le mod√®le llama3.2:1b
- ‚úÖ Teste le fonctionnement

### Option 2 : Utiliser le script de d√©ploiement complet

```bash
# Sur votre Raspberry Pi
cd /Users/mathisdupont/ScanGRID

# Red√©ploiement complet (inclut Ollama)
./redeploy_full.sh
```

Ce script fait √©galement :
- Git pull du dernier code
- Build du frontend
- Installation des d√©pendances backend
- **Configuration automatique d'Ollama**
- Migration de la base de donn√©es
- Red√©marrage PM2

## üì¶ Installation Manuelle (si besoin)

### 1. Installer Ollama

```bash
# Installer Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# V√©rifier l'installation
ollama --version
```

### 2. T√©l√©charger le mod√®le llama3.2:1b

```bash
# T√©l√©charger le mod√®le l√©ger (env. 1.3 GB)
ollama pull llama3.2:1b

# V√©rifier que le mod√®le est install√©
ollama list
```

### 3. Installer la d√©pendance Python

```bash
cd /Users/mathisdupont/ScanGRID/backend

# Installer les nouvelles d√©pendances
pip install -r requirements.txt

# Ou directement
pip install ollama
```

### 4. D√©marrer le service Ollama

```bash
# Option 1 : D√©marrage manuel (pour test)
ollama serve

# Option 2 : Service systemd (recommand√© pour production)
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama
```

## üß™ Test de la fonctionnalit√©

### Test backend direct

```bash
# V√©rifier que le mod√®le r√©pond
ollama run llama3.2:1b "Bonjour, es-tu fonctionnel ?"

# Tester avec un prompt technique
ollama run llama3.2:1b "D√©cris en 20 mots un relais 5V SRD-05VDC-SL-C pour domotique"
```

### Test de l'endpoint API

```bash
# Red√©marrer le backend
cd /Users/mathisdupont/ScanGRID/backend
python main.py

# Tester l'endpoint (dans un autre terminal)
curl -X POST "http://localhost:8000/api/improve-description?title=Relais%205V%20SRD-05VDC-SL-C&content=10A%20250VAC%2C%20bobine%205V&instruction=Montage%20sur%20PCB%20pour%20domotique"
```

R√©ponse attendue :
```json
{
  "improved_description": "Relais de puissance 10A pour commutation de charges AC. Pilotage en 5V id√©al pour l'isolation galvanique en domotique.",
  "model": "llama3.2:1b"
}
```

## üé® Utilisation dans l'interface

1. Ouvrir l'√©diteur d'une bo√Æte (cliquer sur une bo√Æte existante ou en cr√©er une)
2. Entrer un **Titre** (requis)
3. √âventuellement remplir la **Description** existante ou les **Articles contenus**
4. Cliquer sur le bouton **‚ú® Am√©liorer avec IA**
5. Attendre quelques secondes (indicateur de chargement)
6. La description am√©lior√©e remplace l'ancienne

## ‚öôÔ∏è Configuration avanc√©e

### Modifier les param√®tres du mod√®le

Dans [backend/main.py](backend/main.py#L510), ajuster :

```python
response = ollama.generate(
    model='llama3.2:1b',
    prompt=prompt,
    options={
        'temperature': 0.2,    # Plus bas = plus factuel (0.0-1.0)
        'num_predict': 40,     # Nombre max de tokens g√©n√©r√©s
        'top_p': 0.9           # Diversit√© (0.0-1.0)
    }
)
```

### Utiliser un mod√®le plus puissant (si Pi 5 ou serveur)

```bash
# T√©l√©charger un mod√®le plus gros (3B param√®tres, ~2GB)
ollama pull llama3.2:3b

# Modifier dans main.py
model='llama3.2:3b'
```

## üîß D√©pannage

### Le script setup_ollama.sh √©choue

```bash
# V√©rifier les permissions
chmod +x setup_ollama.sh

# Lancer avec plus de logs
bash -x setup_ollama.sh
```

### Erreur "Service Ollama non disponible"

```bash
# V√©rifier qu'Ollama tourne
ps aux | grep ollama

# Red√©marrer si n√©cessaire
sudo systemctl restart ollama
```

### Erreur "model not found"

```bash
# Re-t√©l√©charger le mod√®le
ollama pull llama3.2:1b
```

### Timeout ou lenteur

- V√©rifier la RAM disponible : `free -h`
- R√©duire `num_predict` dans le code (ex: 30 au lieu de 40)
- Utiliser le mod√®le 1B au lieu du 3B

### Le bouton est gris√©

- V√©rifier qu'un **titre** est entr√© (requis)
- V√©rifier que le backend est d√©marr√©
- Consulter la console navigateur (F12) pour voir les erreurs

## üìä Performance

Sur Raspberry Pi 4B (4GB RAM) :
- Temps de r√©ponse : **2-5 secondes**
- RAM utilis√©e : **~800 MB**
- CPU : **~50% pendant la g√©n√©ration**

Sur Raspberry Pi 5 (8GB RAM) :
- Temps de r√©ponse : **1-3 secondes**
- RAM utilis√©e : **~1 GB**
- CPU : **~40% pendant la g√©n√©ration**

## üîí S√©curit√©

- ‚úÖ L'IA tourne **100% localement** (pas d'envoi de donn√©es externes)
- ‚úÖ Pas besoin de cl√© API
- ‚úÖ Respect de la vie priv√©e
- ‚úÖ Fonctionne hors ligne

## üìù Exemple de prompt syst√®me

Le prompt utilis√© actuellement :

```
Tu es un assistant technique sp√©cialis√© dans l'inventaire de composants √©lectroniques.

G√©n√®re une description ultra-concise (maximum 25 mots) √† partir des informations suivantes :

Titre : Relais 5V SRD-05VDC-SL-C
Contenu : 10A 250VAC, bobine 5V
Consigne : Montage sur PCB pour domotique

R√®gles strictes :
- Style : Direct, factuel, sans adjectifs marketing
- Structure : [Fonction principale] + [Caract√©ristique cl√©] + [Usage cible]
- Format : Une seule phrase ou deux segments courts s√©par√©s par un point
- Si le contenu est vide ou contradictoire, base-toi uniquement sur le titre
- N'invente pas de sp√©cifications techniques non fournies


## üìú Scripts disponibles

| Script | Description |
|--------|-------------|
| `setup_ollama.sh` | Installation et configuration compl√®te d'Ollama |
| `launch.sh` | Lancement en d√©veloppement (avec v√©rification Ollama) |
| `deploy_raspberry.sh` | D√©ploiement sur Raspberry Pi (inclut Ollama) |
| `redeploy_full.sh` | Red√©ploiement complet avec git pull (inclut Ollama) |

### Ordre recommand√© pour la premi√®re installation

```bash
# 1. Installation initiale d'Ollama
./setup_ollama.sh

# 2. Premier d√©ploiement complet
./redeploy_full.sh

# 3. Pour les red√©ploiements suivants
./redeploy_full.sh  # ou
pm2 reload ecosystem.config.js
```
Description :
```

## üöÄ Prochaines √©tapes possibles

- [ ] Ajouter un cache des descriptions g√©n√©r√©es
- [ ] Permettre la s√©lection du mod√®le dans l'UI
- [ ] Ajouter un historique des versions de descriptions
- [ ] Support multilingue (anglais/fran√ßais)
- [ ] Fine-tuning du mod√®le sur vos composants sp√©cifiques
